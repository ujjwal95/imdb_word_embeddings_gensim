{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change from Theano default to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported tensorflow\n"
     ]
    }
   ],
   "source": [
    "# Select the framework\n",
    "import os\n",
    "import sys\n",
    "# Select the backend\n",
    "#os.environ[“KERAS_BACKEND”] = “theano”\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "# Un-load the old module\n",
    "for mod in sys.modules.keys():\n",
    "    if mod.startswith('keras.'):\n",
    "        del sys.modules[mod]\n",
    "        del sys.modules[\"eras\"]\n",
    "\n",
    "# Import the required modules\n",
    "import keras\n",
    "print('Imported tensorflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim-3.2.0-py3.6-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into test and train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_WORDS=5000 # only use top 10000 words\n",
    "INDEX_FROM=3   # word index offset\n",
    "\n",
    "train,test = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "train_x,train_y = train\n",
    "test_x,test_y = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionaries to convert IMDB reviews to words for Word2Vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "\n",
    "#appended start and end and padding to get a sense of reviews starting and ending\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing review list for Word2Vec training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_list = []\n",
    "for i in range(0,len(train_x)):\n",
    "    review = ' '.join(id_to_word[id] for id in train_x[i])\n",
    "    reviews_list.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(test_x)):\n",
    "    review = ' '.join(id_to_word[id] for id in test_x[i])\n",
    "    reviews_list.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of reviews:50000\n"
     ]
    }
   ],
   "source": [
    "print('Length of reviews:' + str(len(reviews_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1 : \n",
      " <START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "print('Review 1 : \\n ' + str(reviews_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sentences to train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fixed max review length at 200\n",
    "max_sentence_len = 200\n",
    "\n",
    "sentences = [[word for word in doc.lower().translate(str.maketrans('','',string.punctuation)).split()[:max_sentence_len]] for doc in reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Length : 189\n"
     ]
    }
   ],
   "source": [
    "print('Sample Length : ' + str(len(sentences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [[x for x in i if x] for i in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 50000\n"
     ]
    }
   ],
   "source": [
    "print('Num sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word2Vec and checking similar words to some commonly occuring words in the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word2vec...\n",
      "Result embedding shape: (4965, 100)\n",
      "Checking similar words:\n",
      "  beautiful -> gorgeous (0.82), lovely (0.77), stunning (0.72), wonderful (0.67), breathtaking (0.65), fabulous (0.62), magnificent (0.62), elegant (0.62)\n",
      "  worst -> best (0.82), funniest (0.80), greatest (0.76), finest (0.67), weakest (0.61), biggest (0.61), most (0.57), worse (0.45)\n",
      "  bad -> horrible (0.78), terrible (0.77), good (0.77), lousy (0.74), awful (0.74), lame (0.71), stupid (0.70), cheesy (0.64)\n",
      "  great -> wonderful (0.83), terrific (0.83), fantastic (0.82), fine (0.76), superb (0.75), good (0.74), brilliant (0.73), marvelous (0.72)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print('Training word2vec...')\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['beautiful', 'worst', 'bad', 'great']:\n",
    "    most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
    "    print('  %s -> %s' % (word, most_similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#return index of word from word2vec\n",
    "def word2idx(word):\n",
    "    return word_model.wv.vocab[word].index\n",
    "\n",
    "#return word corresponding to index from word2vec\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert from word list to index list by word2vec as embedding layer is learnt according to word2vec weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_list = []\n",
    "for i in range(0,len(train_x)):\n",
    "    review = ' '.join(id_to_word[id] for id in train_x[i])\n",
    "    train_x_list.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_list = [[word for word in doc.lower().translate(str.maketrans('','',string.punctuation)).split()[:max_sentence_len]] for doc in train_x_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the data for LSTM...\n",
      "train_x shape: (25000, 200)\n",
      "train_y shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the data for LSTM...')\n",
    "# train_x = np.zeros([len(train_x_list), max_sentence_len], dtype=np.int32)\n",
    "train_x_list = [[word2idx(i) for i in j] for j in train_x_list]\n",
    "b = np.zeros([len(train_x_list),len(max(train_x_list,key = lambda x: len(x)))])\n",
    "for i,j in enumerate(train_x_list):\n",
    "    b[i][0:len(j)] = j\n",
    "train_x_list = b\n",
    "print('train_x shape:', train_x_list.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_list = []\n",
    "for i in range(0,len(train_x)):\n",
    "    review = ' '.join(id_to_word[id] for id in test_x[i])\n",
    "    test_x_list.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x_list = [[word for word in doc.lower().translate(str.maketrans('','',string.punctuation)).split()[:max_sentence_len]] for doc in test_x_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the data for LSTM...\n",
      "test_x shape: (25000, 200)\n",
      "test_y shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the data for LSTM...')\n",
    "# train_x = np.zeros([len(train_x_list), max_sentence_len], dtype=np.int32)\n",
    "test_x_list = [[word2idx(i) for i in j] for j in test_x_list]\n",
    "b = np.zeros([len(test_x_list),len(max(train_x_list,key = lambda x: len(x)))])\n",
    "for i,j in enumerate(test_x_list):\n",
    "    b[i][0:len(j)] = j\n",
    "test_x_list = b\n",
    "print('test_x shape:', test_x_list.shape)\n",
    "print('test_y shape:', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries for model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking sizes of embedding matrix to be prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4965, 100)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, emdedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4965, 100)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         496500    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 32)          9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 559,433\n",
      "Trainable params: 559,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding( input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.6149 - acc: 0.6500\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 84s 3ms/step - loss: 0.4466 - acc: 0.7996\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 80s 3ms/step - loss: 0.3590 - acc: 0.8446\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.3180 - acc: 0.8649\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.2948 - acc: 0.8766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x69ebf940>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x_list, train_y, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.26%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(test_x_list, test_y, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
